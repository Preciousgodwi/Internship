{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05598984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daf3bc1d",
   "metadata": {},
   "source": [
    "## answer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f1edc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\.conda\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\.conda\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\.conda\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\.conda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\.conda\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\.conda\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\.conda\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\.conda\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\.conda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\.conda\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dell\\.conda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\dell\\.conda\\lib\\site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe3e30",
   "metadata": {},
   "source": [
    "## Answer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c31cf1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Job Location, Company Name, Experience Required]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.shine.com/job-search/data-analyst-jobs-in-bangalore\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    job_listings = soup.find_all(\"li\", class_=\"srlLi\")\n",
    "\n",
    "\n",
    "    for listing in job_listings[:10]:\n",
    "        job_title=[]\n",
    "        job_title = listing.find(\"h2\", class_=\"srlHeading\").text.strip()\n",
    "        job_titles.append(job_title)\n",
    "\n",
    "        job_location=[]\n",
    "        job_location = listing.find(\"span\", class_=\"srlCompLoc\").text.strip()\n",
    "        job_locations.append(job_location)\n",
    "\n",
    "        company_name=[]\n",
    "        company_name = listing.find(\"span\", class_=\"srlCName\").text.strip()\n",
    "        company_names.append(company_name)\n",
    "\n",
    "        experirnce_required=[]\n",
    "        experience = listing.find(\"span\", class_=\"srlExp\").text.strip()\n",
    "        experience_required.append(experience)\n",
    "\n",
    "\n",
    "    data = {\n",
    "        \"Job Title\": job_titles,\n",
    "        \"Job Location\": job_locations,\n",
    "        \"Company Name\": company_names,\n",
    "        \"Experience Required\": experience_required}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b588fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a643a1a6",
   "metadata": {},
   "source": [
    "## Answer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b834b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Job Location, Company Name]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.shine.com/job-search/data-scientist-jobs-in-bangalore\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    job_listings = soup.find_all(\"li\", class_=\"srlLi\")\n",
    "\n",
    "\n",
    "    for listing in job_listings[:10]:\n",
    "        job_title=[]\n",
    "        job_title = listing.find(\"h2\", class_=\"srlHeading\").text.strip()\n",
    "        job_titles.append(job_title)\n",
    "                         \n",
    "        job_location=[]\n",
    "        job_location = listing.find(\"span\", class_=\"srlCompLoc\").text.strip()\n",
    "        job_locations.append(job_location)\n",
    "\n",
    "        company_name=[]\n",
    "        company_name = listing.find(\"span\", class_=\"srlCName\").text.strip()\n",
    "        company_names.append(company_name)\n",
    "\n",
    "    data = {\n",
    "        \"Job Title\": job_titles,\n",
    "        \"Job Location\": job_locations,\n",
    "        \"Company Name\": company_names}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd0f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6e5b14c",
   "metadata": {},
   "source": [
    "## Answer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb462f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.shine.com/job-search/data-scientist-jobs\"\n",
    "url\n",
    " \n",
    "payload = {\n",
    "    \"query\": \"Data Scientist\",\n",
    "    \"loc\": \"Delhi/NCR\",\n",
    "    \"minexp\": \"3\",\n",
    "    \"maxexp\": \"6\"}\n",
    "\n",
    "\n",
    "response = requests.get(url, data=payload)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    \n",
    "    job_listings = soup.find_all(\"li\", class_=\"srlLi\")\n",
    "\n",
    "    \n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "    experience_required = []\n",
    "\n",
    "    \n",
    "    for listing in job_listings[:10]:\n",
    "        \n",
    "        job_title = listing.find(\"h2\", class_=\"srlHeading\").text.strip()\n",
    "        job_titles.append(job_title)\n",
    "\n",
    "        \n",
    "        job_location = listing.find(\"span\", class_=\"srlCompLoc\").text.strip()\n",
    "        job_locations.append(job_location)\n",
    "\n",
    "        \n",
    "        company_name = listing.find(\"span\", class_=\"srlCName\").text.strip()\n",
    "        company_names.append(company_name)\n",
    "\n",
    "        \n",
    "        experience = listing.find(\"span\", class_=\"srlExp\").text.strip()\n",
    "        experience_required.append(experience)\n",
    "\n",
    "\n",
    "    data = {\n",
    "        \"Job Title\": job_titles,\n",
    "        \"Job Location\": job_locations,\n",
    "        \"Company Name\": company_names,\n",
    "        \"Experience Required\": experience_required,\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb5020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f685c260",
   "metadata": {},
   "source": [
    "## Answer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b338f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "base_url = \"https://www.flipkart.com/\"\n",
    "base_url\n",
    "\n",
    "\n",
    "page_number = 1\n",
    "\n",
    "max_sunglasses = 100\n",
    "\n",
    "while len(brands) < max_sunglasses:\n",
    "\n",
    "    search_url = f\"{base_url}/search?q=sunglasses&page={page_number}\"\n",
    "\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        \n",
    "        sunglasses_listings = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "\n",
    "    \n",
    "        for listing in sunglasses_listings:\n",
    "            \n",
    "            brand = listing.find(\"div\", class_=\"_2WkVRV\").text.strip()\n",
    "            brands.append(brand)\n",
    "\n",
    "        \n",
    "            description = listing.find(\"a\", class_=\"IRpwTa\").text.strip()\n",
    "            descriptions.append(description)\n",
    "\n",
    "            \n",
    "            price = listing.find(\"div\", class_=\"_30jeq3\").text.strip()\n",
    "            prices.append(price)\n",
    "\n",
    "            \n",
    "            if len(brands) >= max_sunglasses:\n",
    "                break\n",
    "\n",
    "    \n",
    "        page_number += 1\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        break\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"Brand\": brands,\n",
    "    \"Product Description\": descriptions,\n",
    "    \"Price\": prices,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(df.head(100))  \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "\n",
    "base_url = \"https://www.flipkart.com/\"\n",
    "base_url\n",
    "\n",
    "\n",
    "page_number = 1\n",
    "\n",
    "max_sunglasses = 100\n",
    "\n",
    "\n",
    "while len(brands) < max_sunglasses:\n",
    "    \n",
    "    search_url = f\"{base_url}/search?q=sunglasses&page={page_number}\"\n",
    "\n",
    "    \n",
    "    response = requests.get(search_url)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        \n",
    "        sunglasses_listings = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "\n",
    "        \n",
    "        for listing in sunglasses_listings:\n",
    "            \n",
    "            brand = listing.find(\"div\", class_=\"_2WkVRV\").text.strip()\n",
    "            brands.append(brand)\n",
    "\n",
    "            \n",
    "            description = listing.find(\"a\", class_=\"IRpwTa\").text.strip()\n",
    "            descriptions.append(description)\n",
    "\n",
    "            \n",
    "            price = listing.find(\"div\", class_=\"_30jeq3\").text.strip()\n",
    "            prices.append(price)\n",
    "\n",
    "            \n",
    "            if len(brands) >= max_sunglasses:\n",
    "                break\n",
    "\n",
    "        \n",
    "        page_number += 1\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        break\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"Brand\": brands,\n",
    "    \"Product Description\": descriptions,\n",
    "    \"Price\": prices,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head(100))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abed052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffaf11a9",
   "metadata": {},
   "source": [
    "## Answer 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "review_ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []\n",
    "\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "url\n",
    "\n",
    "max_reviews = 100\n",
    "\n",
    "reviews_scraped = 0\n",
    "while reviews_scraped < max_reviews:\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    \n",
    "        review_containers = soup.find_all(\"div\", class_=\"_27M-vq\")\n",
    "        for container in review_containers:\n",
    "            \n",
    "            rating = container.find(\"div\", class_=\"_3LWZlK\").text.strip()\n",
    "            review_ratings.append(rating)\n",
    "\n",
    "        \n",
    "            summary = container.find(\"p\", class_=\"_2-N8zT\").text.strip()\n",
    "            review_summaries.append(summary)\n",
    "\n",
    "        \n",
    "            review_text = container.find(\"div\", class_=\"t-ZTKy\").text.strip()\n",
    "            full_reviews.append(review_text)\n",
    "\n",
    "        \n",
    "            reviews_scraped += 1\n",
    "\n",
    "        \n",
    "            if reviews_scraped >= max_reviews:\n",
    "                break\n",
    "\n",
    "        \n",
    "        next_page = soup.find(\"a\", class_=\"_1LKTO3\")\n",
    "\n",
    "        \n",
    "        if next_page:\n",
    "            url = \"https://www.flipkart.com\" + next_page[\"href\"]\n",
    "        else:\n",
    "            break  \n",
    "\n",
    "data = {\n",
    "    \"Rating\": review_ratings,\n",
    "    \"Review Summary\": review_summaries,\n",
    "    \"Full Review\": full_reviews,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7da6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12c8edf0",
   "metadata": {},
   "source": [
    "## Answer 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfe23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "sneaker_brands = []\n",
    "sneaker_descriptions = []\n",
    "sneaker_prices = []\n",
    "\n",
    "\n",
    "base_url = \"https://www.flipkart.com/\"\n",
    "base_url\n",
    "\n",
    "page_number = 1\n",
    "\n",
    "max_sneakers = 100\n",
    "\n",
    "\n",
    "while len(sneaker_brands) < max_sneakers:\n",
    "    \n",
    "    search_url = f\"{base_url}/search?q=sneakers&page={page_number}\"\n",
    "\n",
    "    \n",
    "    response = requests.get(search_url)\n",
    "\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    \n",
    "        sneaker_listings = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "\n",
    "        \n",
    "        for listing in sneaker_listings:\n",
    "            \n",
    "            brand = listing.find(\"div\", class_=\"_2WkVRV\").text.strip()\n",
    "            sneaker_brands.append(brand)\n",
    "\n",
    "            \n",
    "            description = listing.find(\"a\", class_=\"IRpwTa\").text.strip()\n",
    "            sneaker_descriptions.append(description)\n",
    "\n",
    "            \n",
    "            price = listing.find(\"div\", class_=\"_30jeq3\").text.strip()\n",
    "            sneaker_prices.append(price)\n",
    "\n",
    "            \n",
    "            if len(sneaker_brands) >= max_sneakers:\n",
    "                break\n",
    "\n",
    "        \n",
    "        page_number += 1\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        break\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"Brand\": sneaker_brands,\n",
    "    \"Product Description\": sneaker_descriptions,\n",
    "    \"Price\": sneaker_prices,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513e0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf280458",
   "metadata": {},
   "source": [
    "## Answer 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.amazon.in/s?k=Laptop\"\n",
    "url\n",
    "\n",
    "params = {\"field-keywords\": \"Laptop\", \"rh\": \"n:1375425031\"}\n",
    "\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    \n",
    "    laptop_listings = soup.find_all(\"div\", class_=\"s-result-item\")\n",
    "\n",
    "    \n",
    "    for listing in laptop_listings[:10]:\n",
    "        laptop_tittle=[]\n",
    "        title = listing.find(\"span\", class_=\"a-text-normal\").text.strip()\n",
    "        laptop_titles.append(title)\n",
    "\n",
    "        laptop_rating=[]\n",
    "        rating = listing.find(\"span\", class_=\"a-icon-alt\")\n",
    "        if rating:\n",
    "            laptop_ratings.append(rating.text.strip())\n",
    "        else:\n",
    "            laptop_ratings.append(\"Not available\")\n",
    "\n",
    "        laptop_price=[]\n",
    "        price = listing.find(\"span\", class_=\"a-price-whole\")\n",
    "        if price:\n",
    "            laptop_prices.append(price.text.strip())\n",
    "        else:\n",
    "            laptop_prices.append(\"Not available\")\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Title:\", laptop_titles[i])\n",
    "    print(\"Ratings:\", laptop_ratings[i])\n",
    "    print(\"Price:\", laptop_prices[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be841f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edae01b8",
   "metadata": {},
   "source": [
    "## Answer 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://www.azquotes.com/\"\n",
    "url\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    \n",
    "    top_quotes_link = None\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        if \"Top Quotes\" in a.text:\n",
    "            top_quotes_link = a[\"href\"]\n",
    "            break\n",
    "\n",
    "    if top_quotes_link:\n",
    "    \n",
    "        top_quotes_url = url + top_quotes_link\n",
    "        response = requests.get(top_quotes_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        \n",
    "            quotes = []\n",
    "            for quote_elem in soup.find_all(\"div\", class_=\"wrap-block\"):\n",
    "                quote_text = quote_elem.find(\"a\", class_=\"title\").text.strip()\n",
    "                quotes.append(quote_text)\n",
    "                \n",
    "                \n",
    "                authors = []\n",
    "                author_elem = quote_elem.find(\"div\", class_=\"author\")\n",
    "                author_text = author_elem.find(\"a\").text.strip()\n",
    "                authors.append(author_text)\n",
    "                \n",
    "                \n",
    "                types = []\n",
    "                type_elem = quote_elem.find(\"div\", class_=\"kw-item\")\n",
    "                type_text = type_elem.find(\"a\").text.strip()\n",
    "                types.append(type_text)\n",
    "\n",
    "            \n",
    "            for i in range(1000):\n",
    "                print(f\"{i + 1}. Quote: {quotes[i]}\")\n",
    "                print(f\"   Author: {authors[i]}\")\n",
    "                print(f\"   Type: {types[i]}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"Failed to retrieve the 'Top Quotes' page.\")\n",
    "    else:\n",
    "        print(\"No link to 'Top Quotes' found on the main page.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the main page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fd3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "face2c5d",
   "metadata": {},
   "source": [
    "## Answer 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.jagranjosh.com/\"\n",
    "url\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    \n",
    "    gk_option = soup.find(\"a\", text=\"GK\")\n",
    "    if gk_option:\n",
    "        gk_url = gk_option.get(\"href\")\n",
    "\n",
    "        response = requests.get(gk_url)\n",
    "\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            \n",
    "            prime_ministers_link = soup.find(\"a\", text=\"List of all Prime Ministers of India\")\n",
    "            if prime_ministers_link:\n",
    "                prime_ministers_url = prime_ministers_link.get(\"href\")\n",
    "\n",
    "                \n",
    "                response = requests.get(prime_ministers_url)\n",
    "\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                    \n",
    "                    prime_ministers_table = soup.find(\"table\", class_=\"tablestyle2\")\n",
    "\n",
    "                    \n",
    "                    names = []\n",
    "                    born_dead = []\n",
    "                    term_of_office = []\n",
    "                    remarks = []\n",
    "\n",
    "                    \n",
    "                    for row in prime_ministers_table.find_all(\"tr\")[1:]:\n",
    "                        columns = row.find_all(\"td\")\n",
    "\n",
    "                    \n",
    "                        name = columns[0].text.strip()\n",
    "                        birth_death = columns[1].text.strip()\n",
    "                        term = columns[2].text.strip()\n",
    "                        remark = columns[3].text.strip()\n",
    "\n",
    "                        \n",
    "                        names.append(name)\n",
    "                        born_dead.append(birth_death)\n",
    "                        term_of_office.append(term)\n",
    "                        remarks.append(remark)\n",
    "\n",
    "                \n",
    "                    data = {\n",
    "                        \"Name\": names,\n",
    "                        \"Born-Dead\": born_dead,\n",
    "                        \"Term of Office\": term_of_office,\n",
    "                        \"Remarks\": remarks,\n",
    "                    }\n",
    "\n",
    "                    df = pd.DataFrame(data)\n",
    "\n",
    "                    \n",
    "                    print(df)\n",
    "\n",
    "                else:\n",
    "                    print(\"Failed to retrieve the page with the list of Prime Ministers.\")\n",
    "            else:\n",
    "                print(\"Link to 'List of all Prime Ministers of India' not found.\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve the GK page.\")\n",
    "    else:\n",
    "        print(\"GK option not found on the main page.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the main page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f052a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c8ed967",
   "metadata": {},
   "source": [
    "## Answer 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.motor1.com/\"\n",
    "url\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    \n",
    "    search_bar = soup.find(\"input\", id=\"searchbox\")\n",
    "    if search_bar:\n",
    "        search_bar[\"value\"] = \"50 most expensive cars\"\n",
    "\n",
    "        \n",
    "        search_form = soup.find(\"form\", id=\"searchform\")\n",
    "        if search_form:\n",
    "            response = requests.post(url, data={\"search\": \"50 most expensive cars\"})\n",
    "\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                \n",
    "                link = soup.find(\"a\", text=\"50 Most Expensive Cars in the World\")\n",
    "                if link:\n",
    "                    link_url = link.get(\"href\")\n",
    "\n",
    "                    \n",
    "                    response = requests.get(link_url)\n",
    "\n",
    "                    \n",
    "                    if response.status_code == 200:\n",
    "                        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            \n",
    "                        car_table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "                        \n",
    "                        car_names = []\n",
    "                        car_prices = []\n",
    "\n",
    "                        \n",
    "                        for row in car_table.find_all(\"tr\")[1:]:\n",
    "                            columns = row.find_all(\"td\")\n",
    "\n",
    "                            \n",
    "                            name = columns[0].text.strip()\n",
    "                            price = columns[1].text.strip()\n",
    "\n",
    "                            \n",
    "                            car_names.append(name)\n",
    "                            car_prices.append(price)\n",
    "\n",
    "                        \n",
    "                        data = {\n",
    "                            \"Car Name\": car_names,\n",
    "                            \"Price\": car_prices,\n",
    "                        }\n",
    "\n",
    "                        df = pd.DataFrame(data)\n",
    "\n",
    "                        \n",
    "                        print(df)\n",
    "\n",
    "                    else:\n",
    "                        print(\"Failed to retrieve the page with the list of cars.\")\n",
    "                else:\n",
    "                    print(\"Link to '50 Most Expensive Cars in the World' not found.\")\n",
    "            else:\n",
    "                print(\"Search request failed.\")\n",
    "        else:\n",
    "            print(\"Search form not found.\")\n",
    "    else:\n",
    "        print(\"Search bar not found.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the main page.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
